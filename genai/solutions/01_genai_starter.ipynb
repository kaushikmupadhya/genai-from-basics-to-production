{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b938e36a3a85e97d",
      "metadata": {
        "collapsed": false,
        "id": "b938e36a3a85e97d"
      },
      "source": [
        "# From Zero to Production: Entwicklung einer eigenen GenAI LÃ¶sung\n",
        "## Notebook 2: GenAI Starter\n",
        "\n",
        "This notebook is intended to play around with prompting and model parameter settings.\n",
        "\n",
        "During this lesson you will learn how to ...\n",
        "\n",
        "- use diverse roles for prompting\n",
        "- apply different prompting patterns\n",
        "- manipulate the model completion via model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e600adda22789bde",
      "metadata": {
        "collapsed": false,
        "id": "e600adda22789bde"
      },
      "source": [
        "### Set up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b94c0f71909f6ae",
      "metadata": {
        "collapsed": false,
        "id": "4b94c0f71909f6ae"
      },
      "source": [
        "Before we can start, we have to setup the environment and several default values for model name, model parameter and prompts.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cbeeafd26f6782f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbeeafd26f6782f5",
        "outputId": "807b9301-7c7e-4aa6-bb4e-25f5567cca18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on COLAB environment.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Check runtime environment to make sure we are running in a colab environment.\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   COLAB = True\n",
        "   print(\"Running on COLAB environment.\")\n",
        "else:\n",
        "   COLAB = False\n",
        "   print(\"WARNING: Running on LOCAL environment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e22b50bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e22b50bb",
        "outputId": "22f51995-9973-4d79-d90a-bc0e08307207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'workshop-genai-data'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 41 (delta 13), reused 25 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (41/41), 438.10 KiB | 6.64 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the data repository into colab\n",
        "!git clone https://github.com/openknowledge/workshop-genai-data.git\n",
        "PDF_DATA_PATH = \"/content/workshop-genai-data/pdf/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e46c42368eef73ef",
      "metadata": {
        "id": "e46c42368eef73ef"
      },
      "outputs": [],
      "source": [
        "# import colab specific lib to read user data (aka colab managed secrets)\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e292ed72b7fa718",
      "metadata": {
        "id": "e292ed72b7fa718"
      },
      "outputs": [],
      "source": [
        "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model.\n",
        "# Note: GEMINI_API_KEY must be set as COLAB userdata before!\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ee347ba2af215a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ee347ba2af215a9",
        "outputId": "16990648-4d41-4305-f578-5d3e6075bd04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GOOGLE_API_KEY set with a length of 39\n"
          ]
        }
      ],
      "source": [
        "# Double check key settings by printing it out (or at least it length).\n",
        "if GOOGLE_API_KEY:\n",
        "    print(f' GOOGLE_API_KEY set with a length of {len(GOOGLE_API_KEY)}')\n",
        "else:\n",
        "    print(f' ERROR: GOOGLE_API_KEY not set correctly!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bee82e5c07ded99",
      "metadata": {
        "collapsed": false,
        "id": "8bee82e5c07ded99"
      },
      "source": [
        "### Definition of convenient functions  \n",
        "\n",
        "The three following methods will simplify to work with the GEMINI genai model.\n",
        "For details see function documentation.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c8ec0540c47fd175",
      "metadata": {
        "id": "c8ec0540c47fd175"
      },
      "outputs": [],
      "source": [
        "# set default values for model, model parameters and prompt\n",
        "DEFAULT_MODEL = \"gemini-1.5-flash\"\n",
        "DEFAULT_CONFIG_TEMPERATURE = 0.9\n",
        "DEFAULT_CONFIG_TOP_K = 1\n",
        "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200\n",
        "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
        "DEFAULT_USER_PROMPT:str = \" \"\n",
        "\n",
        "def call_genai_model_for_completion(\n",
        "        model_name: str = DEFAULT_MODEL,\n",
        "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
        "        top_k: int = DEFAULT_CONFIG_TOP_K,\n",
        "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS,\n",
        "        system_prompt : str = DEFAULT_SYSTEM_PROMPT,\n",
        "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
        "        file_list: list[str] | None = None,\n",
        "        verbose: bool = False\n",
        "        ):\n",
        "\n",
        "\n",
        "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_name : str, optional [default: DEFAULT_MODEL]\n",
        "        The name of the model to use for the completion\n",
        "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
        "        The temperature of the model\n",
        "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
        "        The number of most recent matches to return\n",
        "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
        "        The maximum number of output tokens to return\n",
        "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
        "        The system prompt to use for the completion\n",
        "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
        "        The user prompt to use for the completion\n",
        "    file_list : [str], optional [default: empty list]\n",
        "    verbose : bool, optional [default: False]\n",
        "        Whether to print details of the completion process or not. Defaults to False\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    completions :\n",
        "        a GenerateContentResponse instance representing the genAI model answer(s)\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        # print out summary of input values / parameters\n",
        "        print(f'Generating answer for following config:')\n",
        "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
        "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
        "        print(f'  - MODEL used:\\n {model_name} (temperature = {temperature}, top_k = {top_k}, max_output_tokens = {max_output_tokens})')\n",
        "\n",
        "    # create generation config\n",
        "    model_config = genai.GenerationConfig(\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k\n",
        "    )\n",
        "\n",
        "    # create genai model with generation config\n",
        "    genai_model = genai.GenerativeModel(\n",
        "        model_name= model_name,\n",
        "        system_instruction= system_prompt,\n",
        "        generation_config= model_config\n",
        "    )\n",
        "\n",
        "    if file_list:\n",
        "        contents = [user_prompt] + file_list\n",
        "    else:\n",
        "        contents = user_prompt\n",
        "\n",
        "    response = genai_model.generate_content(contents)\n",
        "    return response;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f46449b2da08c3c5",
      "metadata": {
        "id": "f46449b2da08c3c5"
      },
      "outputs": [],
      "source": [
        "def print_completion_result(completion_result, full:bool = False):\n",
        "\n",
        "    \"\"\" Prints out the completion.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    completion_result : str\n",
        "        A instance of GenerateContentResponse representing a completion\n",
        "    full : bool, optional [default: False]\n",
        "    Whether to print all details of the completion or only the text. Defaults to False\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(f'\\nANSWER of genAI model: \\n')\n",
        "    if full:\n",
        "        print(completion_result)\n",
        "    else:\n",
        "        print(completion_result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a4a5f3afdcc39d35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4a5f3afdcc39d35",
        "outputId": "a6b9747a-cbbf-41ec-93f9-85f4487b3b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mââââââââââââââââââââââââââââââââââââââ\u001b[0m\u001b[91mâ¸\u001b[0m\u001b[90mâ\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "%pip install PyPDF2\n",
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "\n",
        "    \"\"\" Extract text from a pdf file and return\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pdf_path : str\n",
        "        full qualified path name of the pdf file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    extracted_text :\n",
        "        The extracted text from the pdf file\n",
        "    \"\"\"\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        extracted_text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                extracted_text += text\n",
        "        return extracted_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14034189b3eeaf1b",
      "metadata": {
        "collapsed": false,
        "id": "14034189b3eeaf1b"
      },
      "source": [
        "### Exercise 01: Prompting with roles\n",
        "\n",
        "During this exercise you will how to use the different types of prompts.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "52940e4e56fb425f",
      "metadata": {
        "id": "52940e4e56fb425f"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"What is the most beautiful city in the world?\"\n",
        "system_prompt = \"You are a friendly assistant with a preference for Germany.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749dacb9",
      "metadata": {
        "id": "749dacb9"
      },
      "outputs": [],
      "source": [
        "# TODO Call genai model for completion with\n",
        "# - step 1: no prompt at all\n",
        "# - step 2: user prompt only\n",
        "# - step 3: user prompt and system prompt\n",
        "#\n",
        "# Calling the genai model with user prompt and system prompt\n",
        "# should result in prefering a german city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "99ba098493020806",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "99ba098493020806",
        "outputId": "7dfa11bc-fe67-4c7f-e42a-1b9232416ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "Is there anything I can help you with today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: call genai model without any prompts\n",
        "# TODO\n",
        "#  Call genai model for completion without any prompt.\n",
        "#  Print completion result.\n",
        "response = call_genai_model_for_completion()\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee190ebf1f6260c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "ee190ebf1f6260c5",
        "outputId": "e4dd90d4-2086-46f6-a4f6-dd5fc6aa1578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "That's a fantastic question!  There's no single answer to what the most beautiful city in the world is, because beauty is entirely subjective.  What one person finds beautiful, another might not.  It depends so much on individual preferences â  do you prefer bustling metropolises, charming small towns, historical architecture, stunning natural landscapes, or vibrant modern design?\n",
            "\n",
            "To help you find *your* most beautiful city, I'd love to know:\n",
            "\n",
            "* **What kind of architecture do you enjoy?** (e.g., Gothic, Baroque, Modern, etc.)\n",
            "* **What kind of atmosphere are you looking for?** (e.g., lively and energetic, peaceful and quiet, romantic, historical)\n",
            "* **What kind of activities do you enjoy?** (e.g., museums, nightlife, hiking, food tours)\n",
            "\n",
            "Once I have a better sense of your preferences, I can give you some more tailored recommendations.  But in the meantime\n"
          ]
        }
      ],
      "source": [
        "# Step 2: call genai model with user prompt only\n",
        "# TODO\n",
        "#  Call genai model for completion wit user prompt only.\n",
        "#  Print completion result.\n",
        "response = call_genai_model_for_completion(user_prompt=user_prompt)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "89e423141c25fc76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "89e423141c25fc76",
        "outputId": "3bed0291-b5e7-4baf-c79a-6416a3a7be8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "That's a subjective question, of course!  Beauty is in the eye of the beholder, and what one person finds stunning, another might find unremarkable.  However, if you'll allow a little bias (towards a certain charming country with a rich history and stunning architecture...), I'd have to say that **many** German cities are contenders for the title of most beautiful.\n",
            "\n",
            "Berlin, with its vibrant mix of history and modernity, is captivating.  Munich, nestled in the Bavarian Alps, offers a fairytale-like charm.  Cologne's magnificent cathedral is awe-inspiring.  And then there's Hamburg, with its canals and harbor... the list goes on!\n",
            "\n",
            "Ultimately, the \"most beautiful city\" is the one that resonates most with *you*.  But if you're looking for a starting point for your own exploration of beautiful cities, Germany is an excellent place to begin.  Perhaps you could tell me what kind of beauty you find\n"
          ]
        }
      ],
      "source": [
        "# Step 3: call genai model with system and user prompt\n",
        "# TODO\n",
        "#  Call genai model for completion wit user prompt and system prompt.\n",
        "#  Print completion result.\n",
        "response = call_genai_model_for_completion(system_prompt=system_prompt, user_prompt=user_prompt)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dcd0c8b9529083b",
      "metadata": {
        "collapsed": false,
        "id": "3dcd0c8b9529083b"
      },
      "source": [
        "### Exercise 02: Prompting patterns and best practices\n",
        "\n",
        "In this exercise, you will learn how to apply various prompting best practices to achieve the desired result. See [Prompt Engineering Guide](https://www.promptingguide.ai/techniques) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "207448e7dd2eac5",
      "metadata": {
        "collapsed": false,
        "id": "207448e7dd2eac5"
      },
      "source": [
        "#### Prompting parts\n",
        "\n",
        "To obtain the desired result from the genai model when prompting, try to take the following prompt proportions into account:  \n",
        "\n",
        "- role: \"As what kind of person should the model act?\"\n",
        "- context: \"Are there any additional information that can help the model to answer my question?\"\n",
        "- question: \"What is the task/action/question I ask for?\"\n",
        "- output: \"What kind of output (format) do I expect?\"\n",
        "- example(s): \"Are there any helpful examples the model can use?\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a1b9ede126b90994",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-01T16:16:39.033922Z",
          "start_time": "2024-11-01T16:16:39.028864Z"
        },
        "id": "a1b9ede126b90994"
      },
      "outputs": [],
      "source": [
        "initial_prompt = \"I want to go on holiday. Where should I go?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7b37ac0a679611c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "7b37ac0a679611c8",
        "outputId": "2e72ca22-25fc-499e-94cb-7ee1f594c90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "That's exciting! To give you the best holiday recommendation, I need a little more information.  Tell me about:\n",
            "\n",
            "* **Your budget:** Are you looking for a luxury getaway, a budget-friendly trip, or something in between?\n",
            "* **Your travel style:** Do you prefer relaxing on a beach, exploring cities, adventuring in nature, or something else entirely?\n",
            "* **Your travel companions:** Are you travelling solo, with a partner, with friends, or with family (and if family, what ages)?\n",
            "* **Your preferred climate:** Do you prefer warm sunshine, cool breezes, or something else?\n",
            "* **Your preferred activities:**  Are you interested in historical sites, museums, watersports, hiking, nightlife, food tours, etc.?\n",
            "* **When are you planning to travel?:**  This will greatly affect the weather and availability.\n",
            "\n",
            "Once I have a better idea of your preferences, I can give you some personalized recommendations!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call genai model for completion with initial prompt.\n",
        "response = call_genai_model_for_completion(user_prompt=initial_prompt)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2a79e316124f8c01",
      "metadata": {
        "id": "2a79e316124f8c01"
      },
      "outputs": [],
      "source": [
        "# TODO Create a better prompt following the 'prompting parts' best practices.\n",
        "prompt_role = \"Acting as travel planer,\"\n",
        "prompt_context = \"for a 3-day family trip to Paris with a focus on child-friendly activities,\"\n",
        "prompt_question = \"can you create an itinerary travel plan\"\n",
        "prompt_output = \"including daily schedules and accommodation suggestions?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5c4cf823de2bf085",
      "metadata": {
        "id": "5c4cf823de2bf085"
      },
      "outputs": [],
      "source": [
        "prompt_with_parts = f'{prompt_role} {prompt_context} {prompt_question} {prompt_output}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6824faba13a5d66b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "6824faba13a5d66b",
        "outputId": "34fc14d9-6675-4054-d4fb-26e360079f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "Bonjour!  Let's plan a magical 3-day Parisian adventure for your family! This itinerary focuses on kid-friendly activities, balancing iconic landmarks with fun experiences.\n",
            "\n",
            "**Accommodation Suggestions:**\n",
            "\n",
            "* **Family-friendly Hotel near a Metro Station:** Consider hotels in the Marais district (central, charming, lots of family-friendly restaurants), or near the Latin Quarter (close to the Jardin du Luxembourg). Look for hotels with family rooms or connecting rooms. Booking.com or Expedia are good resources for finding family-friendly options with reviews.  Consider factors like elevator availability (for strollers), cribs/cots if needed, and breakfast inclusion.\n",
            "\n",
            "**Day 1: Fairytales and Fun**\n",
            "\n",
            "* **Morning (9:00 AM):** Start your day at the **Jardin du Luxembourg**. This beautiful garden has a playground, puppet shows (check schedules), pony rides, and plenty of space to run around. Let the kids enjoy the playground and perhaps rent\n"
          ]
        }
      ],
      "source": [
        "# Call genai model for completion with prompt with parts.\n",
        "response = call_genai_model_for_completion(user_prompt=prompt_with_parts)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d7fbc8c0dcba241c",
      "metadata": {
        "id": "d7fbc8c0dcba241c"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "#  Use system prompt to define the systems role in detail in addition\n",
        "#  to get an even better or more specific result.\n",
        "#  - personality: travel web site Wonder-World AI chatbot\n",
        "#  - name: Wonder-World (always greet with your name ;-) )\n",
        "#  - mission:  provide helpful queries for travelers.\n",
        "#  - guardrails:\n",
        "#       - answer with advise only if question complies with mission\n",
        "#       - else say \"Sorry I can't answer that question\"\n",
        "system_prompt = (\n",
        "        \"Hello! You are an AI chatbot for a travel web site named Wonder-World.\"\n",
        "        \"Your mission is to provide helpful queries for travelers.\"\n",
        "        \"Remember that before you answer a question, you must check to see if it complies with your mission.\"\n",
        "        \"If not, you can say, Sorry I can't answer that question.\"\n",
        "        \"Always start your answer with your name.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "16d639f13ffcb0b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "16d639f13ffcb0b0",
        "outputId": "918eea64-7b70-406c-b8c1-1a4257708b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "Hello! I'm Wonder-World, your AI travel assistant.  Here's a possible 3-day itinerary for a family trip to Paris focusing on child-friendly activities.  Remember that travel times between locations aren't included, so adjust the schedule based on your family's pace and preferred mode of transportation (metro, taxi, etc.).\n",
            "\n",
            "**Accommodation Suggestion:**  Consider family-friendly hotels near central Paris with easy access to public transportation.  Look for hotels with amenities like cribs, connecting rooms, or kids' clubs.  Booking.com, Expedia, and other travel websites are good resources for finding options and comparing prices.\n",
            "\n",
            "\n",
            "**Day 1: Fairytales and Fun**\n",
            "\n",
            "* **Morning (9:00 AM):** Start your day at Disneyland Paris!  This is a must-do for many families visiting France.  Pre-book your tickets to avoid long queues.  Focus on Fantasyland and Adventureland, perfect for younger\n"
          ]
        }
      ],
      "source": [
        "# Call genai model for completion with prompt with parts.\n",
        "response = call_genai_model_for_completion(user_prompt=prompt_with_parts, system_prompt = system_prompt)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7286d90b642357",
      "metadata": {
        "collapsed": false,
        "id": "4b7286d90b642357"
      },
      "source": [
        "#### Chain of Thoughts\n",
        "\n",
        "Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365ffa3edfa38177",
      "metadata": {
        "collapsed": false,
        "id": "365ffa3edfa38177"
      },
      "source": [
        "In this exercise we want the genai model to determine if our statement is true or false."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f4b97ae16dbc4122",
      "metadata": {
        "id": "f4b97ae16dbc4122"
      },
      "outputs": [],
      "source": [
        "# This is the statement we want to check (as false)\n",
        "statement_to_evaluate = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7b18e1f02b4e83c4",
      "metadata": {
        "id": "7b18e1f02b4e83c4"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "#  Build up a chain of thoughts to help genai to create the right answer:\n",
        "#\n",
        "#  Use\n",
        "#\n",
        "#   - The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "#\n",
        "#  and a corresponding explanation why this is false to help the genai model to answer\n",
        "#  the following statement correctly:\n",
        "#\n",
        "#   - The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "chain_of_thought = (\n",
        "    \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\\n \"\n",
        "    \"Answer: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False. \\n\")\n",
        "\n",
        "chain_of_thought_prompt = (\n",
        "    chain_of_thought +\n",
        "    statement_to_evaluate +\n",
        "    \"Answer:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c268b37229548273",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "c268b37229548273",
        "outputId": "cb1af9ac-a60f-4f6c-8c52-ef3a22828aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41.  The answer is **False**.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call genai model for completion with chain of thought prompt as user prompt.\n",
        "response = call_genai_model_for_completion(user_prompt=chain_of_thought_prompt)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a782178c771a88b",
      "metadata": {
        "collapsed": false,
        "id": "9a782178c771a88b"
      },
      "source": [
        "#### Few Shot Learning\n",
        "\n",
        "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6521ad849bc695",
      "metadata": {
        "collapsed": false,
        "id": "3c6521ad849bc695"
      },
      "source": [
        "In this exercise we want the genai model to rate a given sentence as positive or negative.\n",
        "Use few shot learning to support the genai model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a9f194dbd0f0b0f9",
      "metadata": {
        "id": "a9f194dbd0f0b0f9"
      },
      "outputs": [],
      "source": [
        "# This is the text to rate (as negative)\n",
        "text_to_rate = \"What a horrible show!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "19f1b394d8d81077",
      "metadata": {
        "id": "19f1b394d8d81077"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "#   Build up a few shot prompt that helps the genai model to determine if a given\n",
        "#   statement is meant positive / negative.\n",
        "few_shot_prompt = (\"This is awesome! // positive \\n\"\n",
        "          \"This is bad! // negative \\n\"\n",
        "          \"Wow that movie was rad! // positive \\n\"\n",
        "          + text_to_rate + \" // \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e66833251dfa71c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "e66833251dfa71c",
        "outputId": "19e9203f-829d-4d7b-c56e-d2e5eca21c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call genai model for completion with few shot prompt as user prompt.\n",
        "response = call_genai_model_for_completion(user_prompt=few_shot_prompt)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc44c1ab22b3056",
      "metadata": {
        "collapsed": false,
        "id": "5dc44c1ab22b3056"
      },
      "source": [
        "### Exercise 03: Models and parameters\n",
        "\n",
        "In this exercise, you will learn how to use the various genai model parameters to customise the result according to your wishes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dd81467de62fe661",
      "metadata": {
        "id": "dd81467de62fe661"
      },
      "outputs": [],
      "source": [
        "# You want to know why the color of the sky is blue.\n",
        "model_parameter_prompt = \"Why is the sky blue?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ec797c2ad1e3d020",
      "metadata": {
        "id": "ec797c2ad1e3d020"
      },
      "outputs": [],
      "source": [
        "# TODO: Play around with different model parameter to get various answers\n",
        "temperature = 0.5\n",
        "top_k = 10\n",
        "max_output_tokens = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "353035766a95a4b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "353035766a95a4b0",
        "outputId": "5b389db1-b49a-4a69-b1e7-d1b110bc61b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "The sky is blue due to a phenomenon called Rayleigh scattering.  Sunlight is made up of all the colors of the rainbow.  When sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).  These molecules are much smaller than the wavelengths of visible light.\n",
            "\n",
            "Rayleigh scattering affects shorter wavelengths of light more strongly than longer wavelengths.  Blue and violet light have the shortest wavelengths, so they are scattered much more than other colors.  This scattered blue light is what we see when we look at the sky.\n",
            "\n",
            "While violet light is actually scattered *more* than blue light, our eyes are more sensitive to blue, and the sun emits slightly less violet light, resulting in the sky appearing blue rather than violet.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call genai model for completion with few shot prompt as user prompt.\n",
        "response = call_genai_model_for_completion(user_prompt=model_parameter_prompt, temperature=temperature, top_k = top_k, max_output_tokens = max_output_tokens)\n",
        "print_completion_result(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377d0d387e5c7ac8",
      "metadata": {
        "collapsed": false,
        "id": "377d0d387e5c7ac8"
      },
      "source": [
        "### Exercise 04: Augmenting\n",
        "\n",
        "In this exercise, you will learn how to use your own documents for augmenting the user prompt. To achieve this, we will first read in the files and then transfer their content to the GenAI model.\n",
        "\n",
        "NOTE: The above-mentioned mechanism works well for content / files smaller tha 20 MB. For larger content you should upload the content to the gemini model in advance. See [genai.upload_file](https://ai.google.dev/gemini-api/docs/document-processing) for detailed information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "bc8bb0361401f655",
      "metadata": {
        "id": "bc8bb0361401f655"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "#   Ask questions about the two short stories (see data/pdf).\n",
        "#\n",
        "#   Play around with the number of files, you provide and try to\n",
        "#   understand the differences of the answers.\n",
        "\n",
        "# Define location of augmenting files.\n",
        "PDF_PATH_1 = PDF_DATA_PATH + \"the-mystery-house-001.pdf\";\n",
        "PDF_PATH_2 = PDF_DATA_PATH + \"the-mystery-house-002.pdf\";\n",
        "\n",
        "# Extract text from augmenting files\n",
        "# This is a suitable solution for files smaller 20 MB.\n",
        "pdf_file_1 = extract_text_from_pdf(PDF_PATH_1)\n",
        "pdf_file_2 = extract_text_from_pdf(PDF_PATH_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8adc4707553696f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "8adc4707553696f1",
        "outputId": "17fd6c43-4a30-4d9f-dbd5-57c92b8b75ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANSWER of genAI model: \n",
            "\n",
            "The main character is David Davidson, and his nickname is Shorty.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define prompt with question about file content\n",
        "augmenting_prompt = \"Who is the main character of the story and what is his nickname?\"\n",
        "\n",
        "# call genai model with prompt and files\n",
        "response = call_genai_model_for_completion(user_prompt=augmenting_prompt, file_list=[pdf_file_1, pdf_file_2])\n",
        "print_completion_result(response)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}